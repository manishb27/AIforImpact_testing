{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09472b4cfe54"
   },
   "source": [
    "# LlamaIndex RAG Workflows using Gemini and Firestore\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/llamaindex_workflows.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Forchestration%2Fllamaindex_workflows.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/orchestration/llamaindex_workflows.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/llamaindex_workflows.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/llamaindex_workflows.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/llamaindex_workflows.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/llamaindex_workflows.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/llamaindex_workflows.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/llamaindex_workflows.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18b1887a153f"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "| Author(s) | [Noa Ben-Efraim](https://github.com/noabenefraim) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dddad16697f"
   },
   "source": [
    "## Overview\n",
    "LlamaIndex workflows are a powerful way to orchestrate complex LLM (large language model) applications. They provide an event-driven framework for building AI systems that go beyond simple question-answering.   \n",
    "\n",
    "Think of a workflow as a series of steps, where each step performs a specific action. These actions can be anything from querying an LLM, to retrieving data from a vector database, to interacting with external APIs. The workflow manages the flow of data between these steps, making it easy to build sophisticated AI applications.   \n",
    "\n",
    "Here's a breakdown of the key concepts:\n",
    "\n",
    "+ Events: These trigger actions within the workflow. For example, a user's query can be an initial event that kicks off the workflow.   \n",
    "+ Steps: These are individual functions decorated with @step that process events and potentially emit new events. Steps are the building blocks of your workflow.   \n",
    "+ Event-driven: This means that the workflow reacts to events as they happen, making it flexible and dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bdceacc7b39"
   },
   "source": [
    "This notebook perform a complex Retrieval Augmented Generation (RAG) workflow using Gemini models and Firestore databases. There are two branches for this workflow:\n",
    "\n",
    "_Branch 1_\n",
    "+ Start Event triggered by providing a data directory to the workflow\n",
    "+ Ingest data using the LlamaIndex `SimpleDirectoryReader`\n",
    "+ Load data in the Firestore Database\n",
    "\n",
    "_Branch 2_\n",
    "+ Start Event triggered by providing a query to the workflow\n",
    "+ The QueryMultiStep Event that breaks down a complex query into sequential sub-questions using Gemini. Then proceeds to answer the sub-questions.\n",
    "+ The sub-questions results are passed to the RerankEvent where given the initial user query, Gemini reranks the returned answers to the sub-questions.\n",
    "+ The reranked chunks are passed to the CreateCitationEvents where citations are added to the sub-questions used to generate the answer.\n",
    "+ An answer is synthesized for the original query and returned to the user.\n",
    "\n",
    "References:\n",
    "+ https://docs.llamaindex.ai/en/stable/examples/workflow/rag/\n",
    "+ https://docs.llamaindex.ai/en/stable/examples/workflow/multi_step_query_engine/\n",
    "+ https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef012cf7cb67"
   },
   "source": [
    "## Get started\n",
    "\n",
    "### Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "323a27d12c02",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q llama-index==\"0.11.8\" \\\n",
    "    llama-index-embeddings-vertex==\"0.2.0\" \\\n",
    "    llama-index-utils-workflow==\"0.2.1\" \\\n",
    "    llama-index-llms-vertex==\"0.3.4\" \\\n",
    "    llama-index-storage-docstore-firestore==\"0.2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f42d12d15616"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f3d98395d9a4",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e114f5653870"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "911453311a5d"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8d7771a5818"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "This notebook requires the following resources:\n",
    "+ Initialized Google Cloud project\n",
    "+ Vertex AI API enabled\n",
    "+ Existing VPC/Subnet\n",
    "+ Existing Firestore database\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "To get started using Firestore Database, refer to the following [documentation](https://cloud.google.com/firestore/docs/manage-databases).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e04ae6146ccd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "\n",
    "import vertexai\n",
    "\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\", isTemplate: true}\n",
    "if PROJECT_ID == \"[your-project-id]\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FIRESTORE_DATABASE_ID = \"kisaan-workflows\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d68eca7a8d4f"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93704f34a080"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9b7824a768bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Any, cast\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import (\n",
    "    ResponseMode,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.schema import MetadataMode, NodeWithScore, QueryBundle, TextNode\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from llama_index.storage.docstore.firestore import FirestoreDocumentStore\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from vertexai.generative_models import HarmBlockThreshold, HarmCategory, SafetySetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e101376082c"
   },
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f15a704ea11b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The process should pull files from a GCS bucket.\n",
    "# temporarily pulls data from a manually created folder in the Jupyter lab environment\n",
    "\n",
    "# !mkdir -p './data'\n",
    "# !wget 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt' -O 'data/gatsby.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "927a7ffd9ad8"
   },
   "source": [
    "### Set credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7d11aac2947b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.auth.transport.requests\n",
    "\n",
    "# credentials will now have an api token\n",
    "credentials = google.auth.default(quota_project_id=PROJECT_ID)[0]\n",
    "request = google.auth.transport.requests.Request()\n",
    "credentials.refresh(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af476af08250"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "456efd13df2a"
   },
   "source": [
    "### Set up the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "695792a24ba9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "safety_config = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    ),\n",
    "]\n",
    "embedding_model = VertexTextEmbedding(\n",
    "    model_name=\"text-embedding-004\", credentials=credentials\n",
    ")\n",
    "llm = Vertex(\n",
    "    model=\"gemini-pro\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=3000,\n",
    "    safety_settings=safety_config,\n",
    "    credentials=credentials,\n",
    ")\n",
    "\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                      Type                   Data/Info\n",
      "--------------------------------------------------------------\n",
      "Any                           _SpecialForm           typing.Any\n",
      "Context                       type                   <class 'llama_index.core.<...>orkflow.context.Context'>\n",
      "Event                         ModelMetaclass         <class 'llama_index.core.workflow.events.Event'>\n",
      "FIRESTORE_DATABASE_ID         str                    kisaan-workflows\n",
      "FirestoreDocumentStore        ABCMeta                <class 'llama_index.stora<...>.FirestoreDocumentStore'>\n",
      "HarmBlockThreshold            ProtoEnumMeta          <enum 'HarmBlockThreshold'>\n",
      "HarmCategory                  ProtoEnumMeta          <enum 'HarmCategory'>\n",
      "LLM                           ModelMetaclass         <class 'llama_index.core.llms.llm.LLM'>\n",
      "LLMRerank                     ModelMetaclass         <class 'llama_index.core.<...>or.llm_rerank.LLMRerank'>\n",
      "LOCATION                      str                    us-central1\n",
      "Markdown                      type                   <class 'IPython.core.display.Markdown'>\n",
      "MetadataMode                  EnumMeta               <enum 'MetadataMode'>\n",
      "NodeWithScore                 ModelMetaclass         <class 'llama_index.core.schema.NodeWithScore'>\n",
      "PROJECT_ID                    str                    kisaan-companion-marketplace\n",
      "PromptTemplate                ModelMetaclass         <class 'llama_index.core.<...>pts.base.PromptTemplate'>\n",
      "QueryBundle                   ABCMeta                <class 'llama_index.core.schema.QueryBundle'>\n",
      "ResponseMode                  EnumMeta               <enum 'ResponseMode'>\n",
      "SafetySetting                 type                   <class 'vertexai.generati<...>ve_models.SafetySetting'>\n",
      "SentenceSplitter              ModelMetaclass         <class 'llama_index.core.<...>ntence.SentenceSplitter'>\n",
      "Settings                      _Settings              _Settings(_llm=Vertex(cal<...>e, _transformations=None)\n",
      "SimpleDirectoryReader         ABCMeta                <class 'llama_index.core.<...>e.SimpleDirectoryReader'>\n",
      "StartEvent                    ModelMetaclass         <class 'llama_index.core.<...>kflow.events.StartEvent'>\n",
      "StepDecomposeQueryTransform   ABCMeta                <class 'llama_index.core.<...>DecomposeQueryTransform'>\n",
      "StopEvent                     ModelMetaclass         <class 'llama_index.core.<...>rkflow.events.StopEvent'>\n",
      "StorageContext                type                   <class 'llama_index.core.<...>_context.StorageContext'>\n",
      "TextNode                      ModelMetaclass         <class 'llama_index.core.schema.TextNode'>\n",
      "VectorStoreIndex              ABCMeta                <class 'llama_index.core.<...>e.base.VectorStoreIndex'>\n",
      "Vertex                        ModelMetaclass         <class 'llama_index.llms.vertex.base.Vertex'>\n",
      "VertexTextEmbedding           ModelMetaclass         <class 'llama_index.embed<...>ase.VertexTextEmbedding'>\n",
      "Workflow                      WorkflowMeta           <class 'llama_index.core.<...>kflow.workflow.Workflow'>\n",
      "cast                          function               <function cast at 0x7eff3f006a70>\n",
      "credentials                   Credentials            <google.auth.compute_engi<...>object at 0x7eff143c7d60>\n",
      "display                       function               <function display at 0x7eff3d640820>\n",
      "draw_all_possible_flows       function               <function draw_all_possib<...>_flows at 0x7efee18384c0>\n",
      "embedding_model               VertexTextEmbedding    model_name='text-embeddin<...>_id=None private_key=None\n",
      "get_response_synthesizer      function               <function get_response_sy<...>esizer at 0x7efee24a5870>\n",
      "google                        module                 <module 'google' (<_froze<...>ject at 0x7eff3f0d92d0>)>\n",
      "llm                           Vertex                 callback_manager=<llama_i<...>al_kwargs={} iscode=False\n",
      "os                            module                 <module 'os' from '/opt/c<...>da/lib/python3.10/os.py'>\n",
      "request                       Request                <google.auth.transport.re<...>object at 0x7efef263c9d0>\n",
      "safety_config                 list                   n=3\n",
      "step                          function               <function step at 0x7efee18d24d0>\n",
      "vertexai                      module                 <module 'vertexai' from '<...>es/vertexai/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b99e7e4aa0d"
   },
   "source": [
    "### Define Event classes\n",
    "\n",
    "Here we will create custom events that can be emitted by steps and trigger other steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "31173e6befe2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    \"\"\"Result of running reranking on retrieved nodes\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "    source_nodes: list[NodeWithScore]\n",
    "    final_response_metadata: dict[str, Any]\n",
    "\n",
    "\n",
    "class FirestoreIndexData(Event):\n",
    "    \"\"\"Result of indexing documents in Firestore\"\"\"\n",
    "\n",
    "    status: str\n",
    "\n",
    "\n",
    "class QueryMultiStepEvent(Event):\n",
    "    \"\"\"\n",
    "    Event containing results of a multi-step query process.\n",
    "\n",
    "    Attributes:\n",
    "        nodes (List[NodeWithScore]): List of nodes with their associated scores.\n",
    "        source_nodes (List[NodeWithScore]): List of source nodes with their scores.\n",
    "        final_response_metadata (Dict[str, Any]): Metadata associated with the final response.\n",
    "    \"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "    source_nodes: list[NodeWithScore]\n",
    "    final_response_metadata: dict[str, Any]\n",
    "\n",
    "\n",
    "class CreateCitationsEvent(Event):\n",
    "    \"\"\"Add citations to the nodes.\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "    source_nodes: list[NodeWithScore]\n",
    "    final_response_metadata: dict[str, Any]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "805eae826756"
   },
   "source": [
    "### Update Prompt Templates\n",
    "\n",
    "Defining custom prompts used for the citation portion of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b2ec8b2859f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CITATION_QA_TEMPLATE = PromptTemplate(\n",
    "    \"Your task is to answer the question based on the information given in the sources listed below.\"\n",
    "    \"Use only the provided sources to answer.\"\n",
    "    \"Cite the source number(s) for any information you use in your answer (e.g., [1]).\"\n",
    "    \"Always include at least one source citation in your answer.\"\n",
    "    \"Only cite a source if you directly use information from it.\"\n",
    "    \"If the sources don't contain the information needed to answer the question, state that.\"\n",
    "    \"For example:\"\n",
    "    \"Source 1: Apples are red, green, or yellow.\"\n",
    "    \"Source 2:  Bananas are yellow when ripe.\"\n",
    "    \"Source 3: Strawberries are red when ripe.\"\n",
    "    \"Query: Which fruits are red when ripe?\"\n",
    "    \"Answer: Apples [1] and strawberries [3] can be red when ripe.\"\n",
    "    \"------\"\n",
    "    \"Below are several numbered sources of information:\"\n",
    "    \"------\"\n",
    "    \"{context_str}\"\n",
    "    \"------\"\n",
    "    \"Query: {query_str}\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "CITATION_REFINE_TEMPLATE = PromptTemplate(\n",
    "    \"You have an initial answer to a query.\"\n",
    "    \"Your job is to improve this answer using the information provided in the numbered sources below. Here's how:\"\n",
    "    \" - Read the existing answer and the sources carefully.\"\n",
    "    \" - Identify any information in the sources that can improve the answer by adding details, making it more accurate, or providing better support.\"\n",
    "    \" - If the sources provide new information, incorporate it into the answer.\"\n",
    "    \" - If the sources contradict the existing answer, correct the answer.\"\n",
    "    \" - If the sources aren't helpful, keep the original answer.\"\n",
    "    \"Cite the source number(s) for any information you use in your answer (e.g., [1]).\"\n",
    "    \"We have provided an existing answer: {existing_answer}\"\n",
    "    \"Below are several numbered sources of information. \"\n",
    "    \"Use them to refine the existing answer. \"\n",
    "    \"If the provided sources are not helpful, you will repeat the existing answer.\"\n",
    "    \"------\"\n",
    "    \"{context_msg}\"\n",
    "    \"------\"\n",
    "    \"Query: {query_str}\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "DEFAULT_CITATION_CHUNK_SIZE = 512\n",
    "DEFAULT_CITATION_CHUNK_OVERLAP = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09009f1cd892"
   },
   "source": [
    "### Workflow Class\n",
    "\n",
    "The RAGWorkflow() class contains all the steps of the workflow. We define the steps by decorating the method with @step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "486b4ba78947",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest_data(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> FirestoreIndexData | None:\n",
    "        \"\"\"Entry point to ingest a document, triggered by a StartEvent with 'dirname'.\"\"\"\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        await ctx.set(\"documents\", documents)\n",
    "        return FirestoreIndexData(\n",
    "            status=\"First step complete. Data loaded into Documents.\"\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def load_database(self, ctx: Context, ev: FirestoreIndexData) -> StopEvent:\n",
    "        print(ev.status)\n",
    "\n",
    "        # create (or load) docstore and add nodes\n",
    "        docstore = FirestoreDocumentStore.from_database(\n",
    "            project=PROJECT_ID,\n",
    "            database=FIRESTORE_DATABASE_ID,\n",
    "        )\n",
    "\n",
    "        docstore.add_documents(await ctx.get(\"documents\"))\n",
    "\n",
    "        # create storage context\n",
    "        storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "        # setup index\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=await ctx.get(\"documents\"), storage_context=storage_context\n",
    "        )\n",
    "\n",
    "        print(\"Index created\")\n",
    "        return StopEvent(index)\n",
    "\n",
    "    def combine_queries(\n",
    "        self,\n",
    "        query_bundle: QueryBundle,\n",
    "        prev_reasoning: str,\n",
    "        llm: LLM,\n",
    "    ) -> QueryBundle:\n",
    "        \"\"\"Combine queries using StepDecomposeQueryTransform.\"\"\"\n",
    "        transform_metadata = {\"prev_reasoning\": prev_reasoning}\n",
    "        return StepDecomposeQueryTransform(llm=llm)(\n",
    "            query_bundle, metadata=transform_metadata\n",
    "        )\n",
    "\n",
    "    def default_stop_fn(self, stop_dict: dict) -> bool:\n",
    "        \"\"\"Stop function for multi-step query combiner.\"\"\"\n",
    "        query_bundle = cast(QueryBundle, stop_dict.get(\"query_bundle\"))\n",
    "        if query_bundle is None:\n",
    "            raise ValueError(\"Response must be provided to stop function.\")\n",
    "\n",
    "        return \"none\" in query_bundle.query_str.lower()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def query_multistep(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> QueryMultiStepEvent | None:\n",
    "        \"\"\"Entry point for RAG, triggered by a StartEvent with `query`. Execute multi-step query process.\"\"\"\n",
    "\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        prev_reasoning = \"\"\n",
    "        cur_response = None\n",
    "        should_stop = False\n",
    "        cur_steps = 0\n",
    "\n",
    "        # use response\n",
    "        final_response_metadata: dict[str, Any] = {\"sub_qa\": []}\n",
    "\n",
    "        text_chunks = []\n",
    "        source_nodes = []\n",
    "\n",
    "        stop_fn = self.default_stop_fn\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        print(f\"Query the database with: {query}\")\n",
    "\n",
    "        # store the query in the global context\n",
    "        await ctx.set(\"query\", query)\n",
    "\n",
    "        # get the index from the global context\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        num_steps = ev.get(\"num_steps\")\n",
    "        query_engine = index.as_query_engine()\n",
    "\n",
    "        while not should_stop:\n",
    "            if num_steps is not None and cur_steps >= num_steps:\n",
    "                should_stop = True\n",
    "                break\n",
    "            elif should_stop:\n",
    "                break\n",
    "\n",
    "            updated_query_bundle = self.combine_queries(\n",
    "                QueryBundle(query_str=query),\n",
    "                prev_reasoning,\n",
    "                llm=Settings.llm,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Created query for the step - {cur_steps} is: {updated_query_bundle}\"\n",
    "            )\n",
    "\n",
    "            stop_dict = {\"query_bundle\": updated_query_bundle}\n",
    "            if stop_fn(stop_dict):\n",
    "                should_stop = True\n",
    "                break\n",
    "\n",
    "            cur_response = query_engine.query(updated_query_bundle)\n",
    "\n",
    "            # append to response builder\n",
    "            cur_qa_text = (\n",
    "                f\"\\nQuestion: {updated_query_bundle.query_str}\\n\"\n",
    "                f\"Answer: {cur_response!s}\"\n",
    "            )\n",
    "            text_chunks.append(cur_qa_text)\n",
    "            print(\"Source nodes used:\\n\")\n",
    "            for source_node in cur_response.source_nodes:\n",
    "                print(source_node)\n",
    "                source_nodes.append(source_node)\n",
    "\n",
    "            # update metadata\n",
    "            final_response_metadata[\"sub_qa\"].append(\n",
    "                (updated_query_bundle.query_str, cur_response)\n",
    "            )\n",
    "\n",
    "            prev_reasoning += (\n",
    "                f\"- {updated_query_bundle.query_str}\\n\" f\"- {cur_response!s}\\n\"\n",
    "            )\n",
    "            cur_steps += 1\n",
    "\n",
    "        nodes = [\n",
    "            NodeWithScore(node=TextNode(text=text_chunk)) for text_chunk in text_chunks\n",
    "        ]\n",
    "        return QueryMultiStepEvent(\n",
    "            nodes=nodes,\n",
    "            source_nodes=source_nodes,\n",
    "            final_response_metadata=final_response_metadata,\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def rerank(self, ctx: Context, ev: QueryMultiStepEvent) -> RerankEvent:\n",
    "        # Rerank the nodes\n",
    "        ranker = LLMRerank(choice_batch_size=5, top_n=10, llm=Settings.llm)\n",
    "        print(\"Entering reranking of nodes:\\n\")\n",
    "        print(\"Original query: \", await ctx.get(\"query\", default=None), flush=True)\n",
    "        # print(await ctx.get(\"query\", default=None), flush=True)\n",
    "        try:\n",
    "            new_nodes = ranker.postprocess_nodes(\n",
    "                ev.nodes, query_str=await ctx.get(\"query\", default=None)\n",
    "            )\n",
    "        except:\n",
    "            # re ranker is not guaranteed to create parsable output\n",
    "            new_nodes = ev.nodes\n",
    "\n",
    "        print(f\"Reranked nodes to {len(new_nodes)}\")\n",
    "        return RerankEvent(\n",
    "            nodes=new_nodes,\n",
    "            source_nodes=ev.source_nodes,\n",
    "            final_response_metadata=ev.final_response_metadata,\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def create_citation_nodes(self, ev: RerankEvent) -> CreateCitationsEvent:\n",
    "        \"\"\"\n",
    "        Modify retrieved nodes to create granular sources for citations.\n",
    "\n",
    "        Takes a list of NodeWithScore objects and splits their content\n",
    "        into smaller chunks, creating new NodeWithScore objects for each chunk.\n",
    "        Each new node is labeled as a numbered source, allowing for more precise\n",
    "        citation in query results.\n",
    "\n",
    "        Args:\n",
    "            nodes (List[NodeWithScore]): A list of NodeWithScore objects to be processed.\n",
    "\n",
    "        Returns:\n",
    "            List[NodeWithScore]: A new list of NodeWithScore objects, where each object\n",
    "            represents a smaller chunk of the original nodes, labeled as a source.\n",
    "        \"\"\"\n",
    "        nodes = ev.nodes\n",
    "\n",
    "        new_nodes: list[NodeWithScore] = []\n",
    "\n",
    "        text_splitter = SentenceSplitter(\n",
    "            chunk_size=DEFAULT_CITATION_CHUNK_SIZE,\n",
    "            chunk_overlap=DEFAULT_CITATION_CHUNK_OVERLAP,\n",
    "        )\n",
    "\n",
    "        for node in nodes:\n",
    "            print(node)\n",
    "\n",
    "            text_chunks = text_splitter.split_text(\n",
    "                node.node.get_content(metadata_mode=MetadataMode.NONE)\n",
    "            )\n",
    "\n",
    "            for text_chunk in text_chunks:\n",
    "                text = f\"Source {len(new_nodes)+1}:\\n{text_chunk}\\n\"\n",
    "\n",
    "                new_node = NodeWithScore(\n",
    "                    node=TextNode.model_validate(node.node), score=node.score\n",
    "                )\n",
    "\n",
    "                new_node.node.text = text\n",
    "                new_nodes.append(new_node)\n",
    "        return CreateCitationsEvent(\n",
    "            nodes=new_nodes,\n",
    "            source_nodes=ev.source_nodes,\n",
    "            final_response_metadata=ev.final_response_metadata,\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: CreateCitationsEvent) -> StopEvent:\n",
    "        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "\n",
    "        print(\"Synthesizing final result...\")\n",
    "\n",
    "        response_synthesizer = get_response_synthesizer(\n",
    "            llm=Vertex(model=\"gemini-1.5-pro\", temperature=0.0, max_tokens=5000),\n",
    "            text_qa_template=CITATION_QA_TEMPLATE,\n",
    "            refine_template=CITATION_REFINE_TEMPLATE,\n",
    "            response_mode=ResponseMode.COMPACT,\n",
    "            use_async=True,\n",
    "        )\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        response = await response_synthesizer.asynthesize(\n",
    "            query, nodes=ev.nodes, additional_source_nodes=ev.source_nodes\n",
    "        )\n",
    "        return StopEvent(result=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "506c0759eab2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_step_workflow.html\n"
     ]
    }
   ],
   "source": [
    "# optional - generate DAG for workflow created above\n",
    "draw_all_possible_flows(workflow=RAGWorkflow, filename=\"multi_step_workflow.html\")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                         Type                   Data/Info\n",
      "-----------------------------------------------------------------\n",
      "Any                              _SpecialForm           typing.Any\n",
      "CITATION_QA_TEMPLATE             PromptTemplate         metadata={'prompt_type': <...>ery: {query_str}Answer: \"\n",
      "CITATION_REFINE_TEMPLATE         PromptTemplate         metadata={'prompt_type': <...>ery: {query_str}Answer: \"\n",
      "Context                          type                   <class 'llama_index.core.<...>orkflow.context.Context'>\n",
      "CreateCitationsEvent             ModelMetaclass         <class '__main__.CreateCitationsEvent'>\n",
      "DEFAULT_CITATION_CHUNK_OVERLAP   int                    20\n",
      "DEFAULT_CITATION_CHUNK_SIZE      int                    512\n",
      "Event                            ModelMetaclass         <class 'llama_index.core.workflow.events.Event'>\n",
      "FIRESTORE_DATABASE_ID            str                    kisaan-workflows\n",
      "FirestoreDocumentStore           ABCMeta                <class 'llama_index.stora<...>.FirestoreDocumentStore'>\n",
      "FirestoreIndexData               ModelMetaclass         <class '__main__.FirestoreIndexData'>\n",
      "HarmBlockThreshold               ProtoEnumMeta          <enum 'HarmBlockThreshold'>\n",
      "HarmCategory                     ProtoEnumMeta          <enum 'HarmCategory'>\n",
      "LLM                              ModelMetaclass         <class 'llama_index.core.llms.llm.LLM'>\n",
      "LLMRerank                        ModelMetaclass         <class 'llama_index.core.<...>or.llm_rerank.LLMRerank'>\n",
      "LOCATION                         str                    us-central1\n",
      "Markdown                         type                   <class 'IPython.core.display.Markdown'>\n",
      "MetadataMode                     EnumMeta               <enum 'MetadataMode'>\n",
      "NodeWithScore                    ModelMetaclass         <class 'llama_index.core.schema.NodeWithScore'>\n",
      "PROJECT_ID                       str                    kisaan-companion-marketplace\n",
      "PromptTemplate                   ModelMetaclass         <class 'llama_index.core.<...>pts.base.PromptTemplate'>\n",
      "QueryBundle                      ABCMeta                <class 'llama_index.core.schema.QueryBundle'>\n",
      "QueryMultiStepEvent              ModelMetaclass         <class '__main__.QueryMultiStepEvent'>\n",
      "RAGWorkflow                      WorkflowMeta           <class '__main__.RAGWorkflow'>\n",
      "RerankEvent                      ModelMetaclass         <class '__main__.RerankEvent'>\n",
      "ResponseMode                     EnumMeta               <enum 'ResponseMode'>\n",
      "RetrieverEvent                   ModelMetaclass         <class '__main__.RetrieverEvent'>\n",
      "SafetySetting                    type                   <class 'vertexai.generati<...>ve_models.SafetySetting'>\n",
      "SentenceSplitter                 ModelMetaclass         <class 'llama_index.core.<...>ntence.SentenceSplitter'>\n",
      "Settings                         _Settings              _Settings(_llm=Vertex(cal<...>e, _transformations=None)\n",
      "SimpleDirectoryReader            ABCMeta                <class 'llama_index.core.<...>e.SimpleDirectoryReader'>\n",
      "StartEvent                       ModelMetaclass         <class 'llama_index.core.<...>kflow.events.StartEvent'>\n",
      "StepDecomposeQueryTransform      ABCMeta                <class 'llama_index.core.<...>DecomposeQueryTransform'>\n",
      "StopEvent                        ModelMetaclass         <class 'llama_index.core.<...>rkflow.events.StopEvent'>\n",
      "StorageContext                   type                   <class 'llama_index.core.<...>_context.StorageContext'>\n",
      "TextNode                         ModelMetaclass         <class 'llama_index.core.schema.TextNode'>\n",
      "VectorStoreIndex                 ABCMeta                <class 'llama_index.core.<...>e.base.VectorStoreIndex'>\n",
      "Vertex                           ModelMetaclass         <class 'llama_index.llms.vertex.base.Vertex'>\n",
      "VertexTextEmbedding              ModelMetaclass         <class 'llama_index.embed<...>ase.VertexTextEmbedding'>\n",
      "Workflow                         WorkflowMeta           <class 'llama_index.core.<...>kflow.workflow.Workflow'>\n",
      "cast                             function               <function cast at 0x7eff3f006a70>\n",
      "credentials                      Credentials            <google.auth.compute_engi<...>object at 0x7eff143c7d60>\n",
      "display                          function               <function display at 0x7eff3d640820>\n",
      "draw_all_possible_flows          function               <function draw_all_possib<...>_flows at 0x7efee18384c0>\n",
      "embedding_model                  VertexTextEmbedding    model_name='text-embeddin<...>_id=None private_key=None\n",
      "get_response_synthesizer         function               <function get_response_sy<...>esizer at 0x7efee24a5870>\n",
      "google                           module                 <module 'google' (<_froze<...>ject at 0x7eff3f0d92d0>)>\n",
      "llm                              Vertex                 callback_manager=<llama_i<...>al_kwargs={} iscode=False\n",
      "os                               module                 <module 'os' from '/opt/c<...>da/lib/python3.10/os.py'>\n",
      "request                          Request                <google.auth.transport.re<...>object at 0x7efef263c9d0>\n",
      "safety_config                    list                   n=3\n",
      "step                             function               <function step at 0x7efee18d24d0>\n",
      "vertexai                         module                 <module 'vertexai' from '<...>es/vertexai/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eebc39a2b70"
   },
   "source": [
    "### Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "d7a98c8d7876",
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = RAGWorkflow(timeout=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0a14595e4e6a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First step complete. Data loaded into Documents.\n",
      "Index created\n"
     ]
    }
   ],
   "source": [
    "# Ingest the documents\n",
    "index = await w.run(dirname=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "439d69a3e7f7"
   },
   "source": [
    "#### Example 1\n",
    "Query: \"What schemes are available to farmers?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "19ebb8696f71",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: What programs are available for farmers?\n",
      "Created query for the step - 0 is: What programs are available for farmers?\n",
      "Source nodes used:\n",
      "\n",
      "Node ID: 60125cbd-d229-4810-ba3e-b4127c119679\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c481   pivotal role in achieving this goal. The\n",
      "Indian government recognizes the vital role of agriculture ...\n",
      "Score:  0.613\n",
      "\n",
      "Node ID: 8d33816d-e377-4ef1-9f80-699ddf6ac4cd\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c482   The Soil Health Card Scheme has played a\n",
      "pivotal role in championing sustainable agriculture and  fo...\n",
      "Score:  0.596\n",
      "\n",
      "Created query for the step - 1 is: ## New Question: \n",
      "\n",
      "What specific types of financial support programs are available for farmers in India? \n",
      "\n",
      "Source nodes used:\n",
      "\n",
      "Node ID: 60125cbd-d229-4810-ba3e-b4127c119679\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c481   pivotal role in achieving this goal. The\n",
      "Indian government recognizes the vital role of agriculture ...\n",
      "Score:  0.667\n",
      "\n",
      "Node ID: cf60f0e6-0a7b-4d3b-a1dc-577a9b740f94\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c480    A Review On Farm er Welfare Schemes In\n",
      "India -  Impacts And Challenges   GUDEM HARIKRISHNA   Resear...\n",
      "Score:  0.665\n",
      "\n",
      "Created query for the step - 2 is: What are the specific types of crop insurance programs available for farmers in India? \n",
      "\n",
      "Source nodes used:\n",
      "\n",
      "Node ID: 8d33816d-e377-4ef1-9f80-699ddf6ac4cd\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c482   The Soil Health Card Scheme has played a\n",
      "pivotal role in championing sustainable agriculture and  fo...\n",
      "Score:  0.632\n",
      "\n",
      "Node ID: cf60f0e6-0a7b-4d3b-a1dc-577a9b740f94\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c480    A Review On Farm er Welfare Schemes In\n",
      "India -  Impacts And Challenges   GUDEM HARIKRISHNA   Resear...\n",
      "Score:  0.617\n",
      "\n",
      "Created query for the step - 3 is: ## What specific types of crop insurance programs are available for farmers in India? \n",
      "\n",
      "Source nodes used:\n",
      "\n",
      "Node ID: 8d33816d-e377-4ef1-9f80-699ddf6ac4cd\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c482   The Soil Health Card Scheme has played a\n",
      "pivotal role in championing sustainable agriculture and  fo...\n",
      "Score:  0.634\n",
      "\n",
      "Node ID: cf60f0e6-0a7b-4d3b-a1dc-577a9b740f94\n",
      "Text: www.ijcrt.org\n",
      "© 2024  IJCRT  | Volume  12, Issue  4 April  2024  | ISSN:  2320 -2882\n",
      "IJCRT2404275  International Journal of Creative Research Thoughts\n",
      "(IJCRT) www.ijcrt.org  c480    A Review On Farm er Welfare Schemes In\n",
      "India -  Impacts And Challenges   GUDEM HARIKRISHNA   Resear...\n",
      "Score:  0.619\n",
      "\n",
      "Entering reranking of nodes:\n",
      "\n",
      "Original query:  What programs are available for farmers?\n",
      "Reranked nodes to 0\n",
      "Synthesizing final result...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Empty Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run a query\n",
    "NUM_STEPS = 4  # @param {type:\"int\"} represents how many sub-questions generated based on the query\n",
    "result = await w.run(\n",
    "    query=\"What programs are available for farmers?\",\n",
    "    index=index,\n",
    "    num_steps=NUM_STEPS,\n",
    ")\n",
    "\n",
    "display(Markdown(f\"{result}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da168aa48f73"
   },
   "source": [
    "Check the ranked LLM generated sub-question answers used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer to comments for re-ranker\n",
    "\n",
    "- `re ranker is not guaranteed to create parsable output`\n",
    "- requires validation from the LlamaIndex documentation (may not exist in current release of python code)\n",
    "- will look at temporarily disabling re-ranker to see if it bypasses this issue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5dd8dab92106",
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, NUM_STEPS):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_nodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for idx in range(0, NUM_STEPS):\n",
    "    print(result.source_nodes[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79eb31af5944"
   },
   "source": [
    "Check the citations from the original source used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ddf240120c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(NUM_STEPS, len(result.source_nodes)):\n",
    "    print(result.source_nodes[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dca7a130be9"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "llamaindex_workflows.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
